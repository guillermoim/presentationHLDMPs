@InProceedings{Infante2022,
  author       = {Infante, Guillermo and Jonsson, Anders and Gómez, Vicenç},
  title        = {Globally Optimal Hierarchical Reinforcement Learning for Linearly-Solvable Markov Decision Processes},
  year         = {2022},
  month        = {Jun.},
  number       = {6},
  pages        = {6970-6977},
  volume       = {36},
  abstractnote = {We present a novel approach to hierarchical reinforcement learning for linearly-solvable Markov decision processes. Our approach assumes that the state space is partitioned, and defines subtasks for moving between the partitions. We represent value functions on several levels of abstraction, and use the compositionality of subtasks to estimate the optimal values of the states in each partition. The policy is implicitly defined on these optimal value estimates, rather than being decomposed among the subtasks. As a consequence, our approach can learn the globally optimal policy, and does not suffer from non-stationarities induced by high-level decisions. If several partitions have equivalent dynamics, the subtasks of those partitions can be shared. We show that our approach is significantly more sample efficient than that of a flat learner and similar hierarchical approaches when the set of boundary states is smaller than the entire state space.},
  doi          = {10.1609/aaai.v36i6.20655},
  file         = {:Infante2022 - Globally Optimal Hierarchical Reinforcement Learning for Linearly Solvable Markov Decision Processes.pdf:PDF},
  groups       = {Hierarchical RL},
  journal      = {Proceedings of the AAAI Conference on Artificial Intelligence},
  ranking      = {rank5},
  readstatus   = {read},
  url          = {https://ojs.aaai.org/index.php/AAAI/article/view/20655},
}

@InProceedings{Wan2021,
  author    = {Wan, Yi and Naik, Abhishek and Sutton, Richard S},
  booktitle = {Proceedings of the 38th International Conference on Machine Learning},
  title     = {Learning and Planning in Average-Reward Markov Decision Processes},
  year      = {2021},
  editor    = {Meila, Marina and Zhang, Tong},
  month     = {18--24 Jul},
  pages     = {10653--10662},
  publisher = {PMLR},
  series    = {Proceedings of Machine Learning Research},
  volume    = {139},
  abstract  = {We introduce learning and planning algorithms for average-reward MDPs, including 1) the first general proven-convergent off-policy model-free control algorithm without reference states, 2) the first proven-convergent off-policy model-free prediction algorithm, and 3) the first off-policy learning algorithm that converges to the actual value function rather than to the value function plus an offset. All of our algorithms are based on using the temporal-difference error rather than the conventional error when updating the estimate of the average reward. Our proof techniques are a slight generalization of those by Abounadi, Bertsekas, and Borkar (2001). In experiments with an Access-Control Queuing Task, we show some of the difficulties that can arise when using methods that rely on reference states and argue that our new algorithms are significantly easier to use.},
  file      = {:Wan2021 - Learning and Planning in Average Reward Markov Decision Processes.pdf:PDF},
  groups    = {Average reward},
  pdf       = {http://proceedings.mlr.press/v139/wan21a/wan21a.pdf},
  url       = {https://proceedings.mlr.press/v139/wan21a.html},
}

    
@InProceedings{Strens2000,
  author    = {Strens, Malcolm J. A.},
  booktitle = {Proceedings of the Seventeenth International Conference on Machine Learning},
  title     = {A Bayesian Framework for Reinforcement Learning},
  year      = {2000},
  address   = {San Francisco, CA, USA},
  pages     = {943–950},
  publisher = {Morgan Kaufmann Publishers Inc.},
  series    = {ICML '00},
  file      = {:Strens2000 - A Bayesian Framework for Reinforcement Learning.pdf:PDF},
  isbn      = {1558607072},
  numpages  = {8},
}

@InProceedings{Konda1999,
  author    = {Vijay R. Konda and John N. Tsitsiklis},
  booktitle = {Advances in Neural Information Processing Systems 12, {[NIPS} Conference, Denver, Colorado, USA, November 29 - December 4, 1999]},
  title     = {Actor-Critic Algorithms},
  year      = {1999},
  editor    = {Sara A. Solla and Todd K. Leen and Klaus{-}Robert M{\"{u}}ller},
  pages     = {1008--1014},
  publisher = {The {MIT} Press},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl    = {https://dblp.org/rec/conf/nips/KondaT99.bib},
  file      = {:Konda1999 - Actor Critic Algorithms.pdf:PDF},
  timestamp = {Mon, 16 May 2022 15:41:51 +0200},
  url       = {http://papers.nips.cc/paper/1786-actor-critic-algorithms},
}

@Article{Bhandari2021,
  author    = {Jalaj Bhandari and Daniel Russo and Raghav Singal},
  journal   = {Oper. Res.},
  title     = {A Finite Time Analysis of Temporal Difference Learning with Linear Function Approximation},
  year      = {2021},
  number    = {3},
  pages     = {950--973},
  volume    = {69},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl    = {https://dblp.org/rec/journals/ior/BhandariRS21.bib},
  doi       = {10.1287/opre.2020.2024},
  file      = {:Bhandari2021 - A Finite Time Analysis of Temporal Difference Learning with Linear Function Approximation.pdf:PDF},
  timestamp = {Thu, 29 Jul 2021 13:40:57 +0200},
}

@Article{Tsitsiklis1997,
  author    = {John N. Tsitsiklis and Benjamin Van Roy},
  journal   = {{IEEE} Trans. Autom. Control.},
  title     = {An analysis of temporal-difference learning with function approximation},
  year      = {1997},
  number    = {5},
  pages     = {674--690},
  volume    = {42},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl    = {https://dblp.org/rec/journals/tac/TsitsiklisR97.bib},
  doi       = {10.1109/9.580874},
  file      = {:Tsitsiklis1997 - An Analysis of Temporal Difference Learning with Function Approximation.pdf:PDF},
  timestamp = {Tue, 17 Aug 2021 08:53:15 +0200},
}

@InProceedings{Kakade2001,
  author    = {Sham M. Kakade},
  booktitle = {Advances in Neural Information Processing Systems 14 [Neural Information Processing Systems: Natural and Synthetic, {NIPS} 2001, December 3-8, 2001, Vancouver, British Columbia, Canada]},
  title     = {A Natural Policy Gradient},
  year      = {2001},
  editor    = {Thomas G. Dietterich and Suzanna Becker and Zoubin Ghahramani},
  pages     = {1531--1538},
  publisher = {{MIT} Press},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl    = {https://dblp.org/rec/conf/nips/Kakade01.bib},
  file      = {:Kakade2001 - A Natural Policy Gradient.pdf:PDF},
  timestamp = {Mon, 16 May 2022 15:41:51 +0200},
  url       = {https://proceedings.neurips.cc/paper/2001/hash/4b86abe48d358ecf194c56c69108433e-Abstract.html},
}

@Article{Neu2017,
  author        = {Gergely Neu and Anders Jonsson and Vicen{\c{c}} G{\'{o}}mez},
  journal       = {CoRR},
  title         = {A unified view of entropy-regularized Markov decision processes},
  year          = {2017},
  volume        = {abs/1705.07798},
  archiveprefix = {arXiv},
  bibsource     = {dblp computer science bibliography, https://dblp.org},
  biburl        = {https://dblp.org/rec/journals/corr/NeuJG17.bib},
  eprint        = {1705.07798},
  eprinttype    = {arxiv},
  file          = {:Neu2017 - A Unified View of Entropy Regularized Markov Decision Processes.pdf:PDF},
  groups        = {Average reward, Entropy-regularized MDPs},
  readstatus    = {read},
  timestamp     = {Mon, 13 Aug 2018 16:47:28 +0200},
}

@InProceedings{Wan2021a,
  author    = {Yi Wan and Abhishek Naik and Richard S. Sutton},
  booktitle = {Advances in Neural Information Processing Systems 34: Annual Conference on Neural Information Processing Systems 2021, NeurIPS 2021, December 6-14, 2021, virtual},
  title     = {Average-Reward Learning and Planning with Options},
  year      = {2021},
  editor    = {Marc'Aurelio Ranzato and Alina Beygelzimer and Yann N. Dauphin and Percy Liang and Jennifer Wortman Vaughan},
  pages     = {22758--22769},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl    = {https://dblp.org/rec/conf/nips/WanNS21.bib},
  file      = {:Wan2021a - Average Reward Learning and Planning with Options.pdf:PDF},
  groups    = {Average reward, Hierarchical RL},
  timestamp = {Tue, 03 May 2022 16:20:49 +0200},
  url       = {https://proceedings.neurips.cc/paper/2021/hash/c058f544c737782deacefa532d9add4c-Abstract.html},
}


@Article{Mahadevan1996,
  author    = {Sridhar Mahadevan},
  journal   = {Mach. Learn.},
  title     = {Average Reward Reinforcement Learning: Foundations, Algorithms, and Empirical Results},
  year      = {1996},
  number    = {1-3},
  pages     = {159--195},
  volume    = {22},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl    = {https://dblp.org/rec/journals/ml/Mahadevan96.bib},
  doi       = {10.1023/A:1018064306595},
  file      = {:Mahadevan1996 - Average Reward Reinforcement Learning_ Foundations, Algorithms, and Empirical Results.pdf:PDF},
  groups    = {Average reward},
  timestamp = {Mon, 02 Mar 2020 16:28:50 +0100},
}

@Article{Sutton1999,
  author     = {Richard S. Sutton and Doina Precup and Satinder Singh},
  journal    = {Artif. Intell.},
  title      = {Between MDPs and Semi-MDPs: {A} Framework for Temporal Abstraction in Reinforcement Learning},
  year       = {1999},
  number     = {1-2},
  pages      = {181--211},
  volume     = {112},
  bibsource  = {dblp computer science bibliography, https://dblp.org},
  biburl     = {https://dblp.org/rec/journals/ai/SuttonPS99.bib},
  doi        = {10.1016/S0004-3702(99)00052-1},
  file       = {:Sutton1999 - Between MDPs and Semi MDPs_ a Framework for Temporal Abstraction in Reinforcement Learning.pdf:PDF},
  groups     = {Hierarchical RL},
  readstatus = {read},
  timestamp  = {Tue, 19 Apr 2022 16:03:28 +0200},
}

@InProceedings{FrancoisLavet2019,
  author    = {Vincent Fran{\c{c}}ois{-}Lavet and Yoshua Bengio and Doina Precup and Joelle Pineau},
  booktitle = {The Thirty-Third {AAAI} Conference on Artificial Intelligence, {AAAI} 2019, The Thirty-First Innovative Applications of Artificial Intelligence Conference, {IAAI} 2019, The Ninth {AAAI} Symposium on Educational Advances in Artificial Intelligence, {EAAI} 2019, Honolulu, Hawaii, USA, January 27 - February 1, 2019},
  title     = {Combined Reinforcement Learning via Abstract Representations},
  year      = {2019},
  pages     = {3582--3589},
  publisher = {{AAAI} Press},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl    = {https://dblp.org/rec/conf/aaai/Francois-LavetB19.bib},
  doi       = {10.1609/aaai.v33i01.33013582},
  file      = {:FrancoisLavet2019 - Combined Reinforcement Learning Via Abstract Representations.pdf:PDF},
  timestamp = {Tue, 02 Feb 2021 08:00:22 +0100},
}

@InProceedings{Niekerk2019,
  author     = {Benjamin van Niekerk and Steven D. James and Adam Christopher Earle and Benjamin Rosman},
  booktitle  = {Proceedings of the 36th International Conference on Machine Learning, {ICML} 2019, 9-15 June 2019, Long Beach, California, {USA}},
  title      = {Composing Value Functions in Reinforcement Learning},
  year       = {2019},
  editor     = {Kamalika Chaudhuri and Ruslan Salakhutdinov},
  pages      = {6401--6409},
  publisher  = {{PMLR}},
  series     = {Proceedings of Machine Learning Research},
  volume     = {97},
  bibsource  = {dblp computer science bibliography, https://dblp.org},
  biburl     = {https://dblp.org/rec/conf/icml/NiekerkJER19.bib},
  file       = {:Niekerk2019 - Composing Value Functions in Reinforcement Learning.pdf:PDF},
  groups     = {Hierarchical RL},
  readstatus = {read},
  timestamp  = {Thu, 27 May 2021 19:27:16 +0200},
  url        = {http://proceedings.mlr.press/v97/van-niekerk19a.html},
}

@Article{Schulman2017,
  author        = {John Schulman and Pieter Abbeel and Xi Chen},
  journal       = {CoRR},
  title         = {Equivalence Between Policy Gradients and Soft Q-Learning},
  year          = {2017},
  volume        = {abs/1704.06440},
  archiveprefix = {arXiv},
  bibsource     = {dblp computer science bibliography, https://dblp.org},
  biburl        = {https://dblp.org/rec/journals/corr/SchulmanAC17.bib},
  eprint        = {1704.06440},
  eprinttype    = {arxiv},
  file          = {:Schulman2017 - Equivalence between Policy Gradients and Soft Q Learning.pdf:PDF},
  groups        = {Entropy-regularized MDPs},
  timestamp     = {Mon, 03 Sep 2018 12:15:29 +0200},
}

@InProceedings{Fruit2017,
  author    = {Ronan Fruit and Alessandro Lazaric},
  booktitle = {Proceedings of the 20th International Conference on Artificial Intelligence and Statistics, {AISTATS} 2017, 20-22 April 2017, Fort Lauderdale, FL, {USA}},
  title     = {Exploration-Exploitation in MDPs with Options},
  year      = {2017},
  editor    = {Aarti Singh and Xiaojin (Jerry) Zhu},
  pages     = {576--584},
  publisher = {{PMLR}},
  series    = {Proceedings of Machine Learning Research},
  volume    = {54},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl    = {https://dblp.org/rec/conf/aistats/FruitL17.bib},
  file      = {:Fruit2017 - Exploration Exploitation in MDPs with Options.pdf:PDF},
  groups    = {Hierarchical RL},
  timestamp = {Wed, 29 May 2019 08:41:44 +0200},
  url       = {http://proceedings.mlr.press/v54/fruit17a.html},
}

@InProceedings{Neu2017a,
  author    = {Gergely Neu and Vicen{\c{c}} G{\'{o}}mez},
  booktitle = {Proceedings of the 30th Conference on Learning Theory, {COLT} 2017, Amsterdam, The Netherlands, 7-10 July 2017},
  title     = {Fast rates for online learning in Linearly Solvable Markov Decision Processes},
  year      = {2017},
  editor    = {Satyen Kale and Ohad Shamir},
  pages     = {1567--1588},
  publisher = {{PMLR}},
  series    = {Proceedings of Machine Learning Research},
  volume    = {65},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl    = {https://dblp.org/rec/conf/colt/NeuG17.bib},
  file      = {:Fast rates for online learning in
Linearly Solvable Markov Decision Processes.pdf:PDF},
  groups    = {Entropy-regularized MDPs},
  timestamp = {Mon, 03 Jan 2022 22:17:33 +0100},
  url       = {http://proceedings.mlr.press/v65/neu17a.html},
}

@Article{Barreto2020,
  author    = {Andr{\'{e}} Barreto and Shaobo Hou and Diana Borsa and David Silver and Doina Precup},
  journal   = {Proc. Natl. Acad. Sci. {USA}},
  title     = {Fast reinforcement learning with generalized policy updates},
  year      = {2020},
  number    = {48},
  pages     = {30079--30087},
  volume    = {117},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl    = {https://dblp.org/rec/journals/pnas/BarretoHBSP20.bib},
  doi       = {10.1073/pnas.1907370117},
  file      = {:Barreto2020 - Fast Reinforcement Learning with Generalized Policy Updates.pdf:PDF},
  timestamp = {Mon, 10 Jan 2022 17:07:18 +0100},
}

@InProceedings{Thakoor2022,
  author    = {Shantanu Thakoor and Mark Rowland and Diana Borsa and Will Dabney and R{\'{e}}mi Munos and Andr{\'{e}} Barreto},
  booktitle = {International Conference on Machine Learning, {ICML} 2022, 17-23 July 2022, Baltimore, Maryland, {USA}},
  title     = {Generalised Policy Improvement with Geometric Policy Composition},
  year      = {2022},
  editor    = {Kamalika Chaudhuri and Stefanie Jegelka and Le Song and Csaba Szepesv{\'{a}}ri and Gang Niu and Sivan Sabato},
  pages     = {21272--21307},
  publisher = {{PMLR}},
  series    = {Proceedings of Machine Learning Research},
  volume    = {162},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl    = {https://dblp.org/rec/conf/icml/ThakoorRBDM022.bib},
  file      = {:Thakoor2022 - Generalised Policy Improvement with Geometric Policy Composition.pdf:PDF},
  timestamp = {Tue, 12 Jul 2022 17:36:52 +0200},
  url       = {https://proceedings.mlr.press/v162/thakoor22a.html},
}

@Article{Steccanella2021,
  author        = {Lorenzo Steccanella and Simone Totaro and Anders Jonsson},
  journal       = {CoRR},
  title         = {Hierarchical Representation Learning for Markov Decision Processes},
  year          = {2021},
  volume        = {abs/2106.01655},
  archiveprefix = {arXiv},
  bibsource     = {dblp computer science bibliography, https://dblp.org},
  biburl        = {https://dblp.org/rec/journals/corr/abs-2106-01655.bib},
  eprint        = {2106.01655},
  eprinttype    = {arxiv},
  file          = {:Steccanella2021 - Hierarchical Representation Learning for Markov Decision Processes.pdf:PDF},
  groups        = {Hierarchical RL},
  timestamp     = {Thu, 10 Jun 2021 16:34:18 +0200},
}

@InProceedings{BasSerrano2021,
  author    = {Joan Bas{-}Serrano and Sebastian Curi and Andreas Krause and Gergely Neu},
  booktitle = {The 24th International Conference on Artificial Intelligence and Statistics, {AISTATS} 2021, April 13-15, 2021, Virtual Event},
  title     = {Logistic Q-Learning},
  year      = {2021},
  editor    = {Arindam Banerjee and Kenji Fukumizu},
  pages     = {3610--3618},
  publisher = {{PMLR}},
  series    = {Proceedings of Machine Learning Research},
  volume    = {130},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl    = {https://dblp.org/rec/conf/aistats/Bas-SerranoC0N21.bib},
  file      = {:BasSerrano2021 - Logistic Q Learning.pdf:PDF},
  timestamp = {Wed, 14 Apr 2021 18:58:38 +0200},
  url       = {http://proceedings.mlr.press/v130/bas-serrano21a.html},
}


@InProceedings{Mendez2022,
  author    = {Jorge A. Mendez and Harm van Seijen and Eric Eaton},
  booktitle = {The Tenth International Conference on Learning Representations, {ICLR} 2022, Virtual Event, April 25-29, 2022},
  title     = {Modular Lifelong Reinforcement Learning via Neural Composition},
  year      = {2022},
  publisher = {OpenReview.net},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl    = {https://dblp.org/rec/conf/iclr/MendezSE22.bib},
  file      = {:Mendez2022 - Modular Lifelong Reinforcement Learning Via Neural Composition.pdf:PDF},
  timestamp = {Sat, 20 Aug 2022 01:15:42 +0200},
  url       = {https://openreview.net/forum?id=5XmLzdslFNN},
}

@Article{Amari1998,
  author    = {Shun{-}ichi Amari},
  journal   = {Neural Comput.},
  title     = {Natural Gradient Works Efficiently in Learning},
  year      = {1998},
  number    = {2},
  pages     = {251--276},
  volume    = {10},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl    = {https://dblp.org/rec/journals/neco/Amari98.bib},
  doi       = {10.1162/089976698300017746},
  file      = {:Amari1998 - Natural Gradient Works Efficiently in Learning.pdf:PDF},
  timestamp = {Tue, 01 Sep 2020 13:12:50 +0200},
}

@Article{Mnih2013,
  author        = {Volodymyr Mnih and Koray Kavukcuoglu and David Silver and Alex Graves and Ioannis Antonoglou and Daan Wierstra and Martin A. Riedmiller},
  journal       = {CoRR},
  title         = {Playing Atari with Deep Reinforcement Learning},
  year          = {2013},
  volume        = {abs/1312.5602},
  archiveprefix = {arXiv},
  bibsource     = {dblp computer science bibliography, https://dblp.org},
  biburl        = {https://dblp.org/rec/journals/corr/MnihKSGAWR13.bib},
  eprint        = {1312.5602},
  eprinttype    = {arxiv},
  file          = {:Mnih2013 - Playing Atari with Deep Reinforcement Learning.pdf:PDF},
  keywords      = {dqn},
  timestamp     = {Mon, 13 Aug 2018 16:47:42 +0200},
}

@InProceedings{Sutton1999a,
  author     = {Richard S. Sutton and David A. McAllester and Satinder Singh and Yishay Mansour},
  booktitle  = {Advances in Neural Information Processing Systems 12, {[NIPS} Conference, Denver, Colorado, USA, November 29 - December 4, 1999]},
  title      = {Policy Gradient Methods for Reinforcement Learning with Function Approximation},
  year       = {1999},
  editor     = {Sara A. Solla and Todd K. Leen and Klaus{-}Robert M{\"{u}}ller},
  pages      = {1057--1063},
  publisher  = {The {MIT} Press},
  bibsource  = {dblp computer science bibliography, https://dblp.org},
  biburl     = {https://dblp.org/rec/conf/nips/SuttonMSM99.bib},
  file       = {:Sutton1999a - Policy Gradient Methods for Reinforcement Learning with Function Approximation.pdf:PDF},
  groups     = {Policy gradients},
  readstatus = {read},
  timestamp  = {Mon, 16 May 2022 15:41:51 +0200},
  url        = {http://papers.nips.cc/paper/1713-policy-gradient-methods-for-reinforcement-learning-with-function-approximation},
}

@InProceedings{Todorov2010,
  author     = {Emanuel Todorov},
  booktitle  = {Advances in Neural Information Processing Systems 23: 24th Annual Conference on Neural Information Processing Systems 2010. Proceedings of a meeting held 6-9 December 2010, Vancouver, British Columbia, Canada},
  title      = {Policy gradients in linearly-solvable MDPs},
  year       = {2010},
  editor     = {John D. Lafferty and Christopher K. I. Williams and John Shawe{-}Taylor and Richard S. Zemel and Aron Culotta},
  pages      = {2298--2306},
  publisher  = {Curran Associates, Inc.},
  bibsource  = {dblp computer science bibliography, https://dblp.org},
  biburl     = {https://dblp.org/rec/conf/nips/Todorov10.bib},
  file       = {:Todorov2010 - Policy Gradients in Linearly Solvable MDPs.pdf:PDF},
  groups     = {Entropy-regularized MDPs, Policy gradients},
  readstatus = {read},
  timestamp  = {Mon, 16 May 2022 15:41:51 +0200},
  url        = {https://proceedings.neurips.cc/paper/2010/hash/69421f032498c97020180038fddb8e24-Abstract.html},
}

@Article{Silver2021,
  author    = {David Silver and Satinder Singh and Doina Precup and Richard S. Sutton},
  journal   = {Artif. Intell.},
  title     = {Reward is enough},
  year      = {2021},
  pages     = {103535},
  volume    = {299},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl    = {https://dblp.org/rec/journals/ai/SilverSPS21.bib},
  doi       = {10.1016/j.artint.2021.103535},
  file      = {:Silver2021 - Reward Is Enough.pdf:PDF},
  timestamp = {Tue, 19 Apr 2022 16:03:28 +0200},
}

@Article{Williams1992,
  author    = {Ronald J. Williams},
  journal   = {Mach. Learn.},
  title     = {Simple Statistical Gradient-Following Algorithms for Connectionist Reinforcement Learning},
  year      = {1992},
  pages     = {229--256},
  volume    = {8},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl    = {https://dblp.org/rec/journals/ml/Williams92.bib},
  doi       = {10.1007/BF00992696},
  file      = {:Williams1992 - Simple Statistical Gradient Following Algorithms for Connectionist Reinforcement Learning.pdf:PDF},
  timestamp = {Mon, 02 Mar 2020 16:28:58 +0100},
}

@InProceedings{Barreto2019,
  author     = {Andr{\'{e}} Barreto and Diana Borsa and Shaobo Hou and Gheorghe Comanici and Eser Ayg{\"{u}}n and Philippe Hamel and Daniel Toyama and Jonathan J. Hunt and Shibl Mourad and David Silver and Doina Precup},
  booktitle  = {Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada},
  title      = {The Option Keyboard: Combining Skills in Reinforcement Learning},
  year       = {2019},
  editor     = {Hanna M. Wallach and Hugo Larochelle and Alina Beygelzimer and Florence d'Alch{\'{e}}{-}Buc and Emily B. Fox and Roman Garnett},
  pages      = {13031--13041},
  bibsource  = {dblp computer science bibliography, https://dblp.org},
  biburl     = {https://dblp.org/rec/conf/nips/BarretoBHCAHTHM19.bib},
  file       = {:Barreto2019 - The Option Keyboard_ Combining Skills in Reinforcement Learning.pdf:PDF},
  groups     = {Hierarchical RL},
  readstatus = {read},
  timestamp  = {Mon, 16 May 2022 15:41:51 +0200},
  url        = {https://proceedings.neurips.cc/paper/2019/hash/251c5ffd6b62cc21c446c963c76cf214-Abstract.html},
}

@InProceedings{Schulman2015,
  author    = {John Schulman and Sergey Levine and Pieter Abbeel and Michael I. Jordan and Philipp Moritz},
  booktitle = {Proceedings of the 32nd International Conference on Machine Learning, {ICML} 2015, Lille, France, 6-11 July 2015},
  title     = {Trust Region Policy Optimization},
  year      = {2015},
  editor    = {Francis R. Bach and David M. Blei},
  pages     = {1889--1897},
  publisher = {JMLR.org},
  series    = {{JMLR} Workshop and Conference Proceedings},
  volume    = {37},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl    = {https://dblp.org/rec/conf/icml/SchulmanLAJM15.bib},
  file      = {:Schulman2015 - Trust Region Policy Optimization.pdf:PDF},
  groups    = {Entropy-regularized MDPs, Policy gradients},
  timestamp = {Wed, 29 May 2019 08:41:45 +0200},
  url       = {http://proceedings.mlr.press/v37/schulman15.html},
}

@InProceedings{Todorov2006,
  author     = {Todorov, Emanuel},
  booktitle  = {Advances in Neural Information Processing Systems},
  title      = {Linearly-solvable Markov decision problems},
  year       = {2006},
  editor     = {B. Sch\"{o}lkopf and J. Platt and T. Hoffman},
  publisher  = {MIT Press},
  volume     = {19},
  file       = {:Todorov2006 - Linearly Solvable Markov Decision Problems.pdf:PDF},
  groups     = {Entropy-regularized MDPs},
  readstatus = {read},
  url        = {https://proceedings.neurips.cc/paper/2006/file/d806ca13ca3449af72a1ea5aedbed26a-Paper.pdf},
}

@Book{Sutton2018,
  author    = {Sutton, Richard S. and Barto, Andrew G.},
  publisher = {A Bradford Book},
  title     = {Reinforcement Learning: An Introduction},
  year      = {2018},
  address   = {Cambridge, MA, USA},
  isbn      = {0262039249},
  abstract  = {The significantly expanded and updated new edition of a widely used text on reinforcement learning, one of the most active research areas in artificial intelligence. Reinforcement learning, one of the most active research areas in artificial intelligence, is a computational approach to learning whereby an agent tries to maximize the total amount of reward it receives while interacting with a complex, uncertain environment. In Reinforcement Learning, Richard Sutton and Andrew Barto provide a clear and simple account of the field's key ideas and algorithms. This second edition has been significantly expanded and updated, presenting new topics and updating coverage of other topics. Like the first edition, this second edition focuses on core online learning algorithms, with the more mathematical material set off in shaded boxes. Part I covers as much of reinforcement learning as possible without going beyond the tabular case for which exact solutions can be found. Many algorithms presented in this part are new to the second edition, including UCB, Expected Sarsa, and Double Learning. Part II extends these ideas to function approximation, with new sections on such topics as artificial neural networks and the Fourier basis, and offers expanded treatment of off-policy learning and policy-gradient methods. Part III has new chapters on reinforcement learning's relationships to psychology and neuroscience, as well as an updated case-studies chapter including AlphaGo and AlphaGo Zero, Atari game playing, and IBM Watson's wagering strategy. The final chapter discusses the future societal impacts of reinforcement learning.},
  file      = {:Sutton2018 - Reinforcement Learning_ an Introduction.pdf:PDF},
}

@Article{NguyenTang2022,
  author        = {Thanh Nguyen{-}Tang and Ming Yin and Sunil Gupta and Svetha Venkatesh and Raman Arora},
  journal       = {CoRR},
  title         = {On Instance-Dependent Bounds for Offline Reinforcement Learning with Linear Function Approximation},
  year          = {2022},
  volume        = {abs/2211.13208},
  archiveprefix = {arXiv},
  bibsource     = {dblp computer science bibliography, https://dblp.org},
  biburl        = {https://dblp.org/rec/journals/corr/abs-2211-13208.bib},
  doi           = {10.48550/arXiv.2211.13208},
  eprint        = {2211.13208},
  file          = {:NguyenTang2022 - On Instance Dependent Bounds for Offline Reinforcement Learning with Linear Function Approximation.pdf:PDF},
  groups        = {Reading Group},
  timestamp     = {Tue, 29 Nov 2022 17:41:18 +0100},
}


@Article{ODonoghue2023,
  author        = {Brendan O'Donoghue},
  journal       = {CoRR},
  title         = {Efficient exploration via epistemic-risk-seeking policy optimization},
  year          = {2023},
  volume        = {abs/2302.09339},
  archiveprefix = {arXiv},
  bibsource     = {dblp computer science bibliography, https://dblp.org},
  biburl        = {https://dblp.org/rec/journals/corr/abs-2302-09339.bib},
  doi           = {10.48550/arXiv.2302.09339},
  eprint        = {2302.09339},
  file          = {:ODonoghue2023 - Efficient Exploration Via Epistemic Risk Seeking Policy Optimization.pdf:PDF},
  groups        = {Reading Group},
  timestamp     = {Thu, 23 Feb 2023 16:02:44 +0100},
}

@Article{Cipollone2023,
  author        = {Roberto Cipollone and Giuseppe De Giacomo and Marco Favorito and Luca Iocchi and Fabio Patrizi},
  journal       = {CoRR},
  title         = {Exploiting Multiple Abstractions in Episodic {RL} via Reward Shaping},
  year          = {2023},
  volume        = {abs/2303.00516},
  archiveprefix = {arXiv},
  bibsource     = {dblp computer science bibliography, https://dblp.org},
  biburl        = {https://dblp.org/rec/journals/corr/abs-2303-00516.bib},
  doi           = {10.48550/arXiv.2303.00516},
  eprint        = {2303.00516},
  file          = {:Cipollone2023 - Exploiting Multiple Abstractions in Episodic RL Via Reward Shaping.pdf:PDF},
  ranking       = {rank5},
  readstatus    = {read},
  timestamp     = {Mon, 06 Mar 2023 16:51:26 +0100},
}

@InProceedings{Schramm2023,
  author     = {Schramm, Liam and Deng, Yunfu and Granados, Edgar and Boularias, Abdeslam},
  booktitle  = {Proceedings of The 6th Conference on Robot Learning},
  title      = {USHER: Unbiased Sampling for Hindsight Experience Replay},
  year       = {2023},
  editor     = {Liu, Karen and Kulic, Dana and Ichnowski, Jeff},
  month      = {14--18 Dec},
  pages      = {2073--2082},
  publisher  = {PMLR},
  series     = {Proceedings of Machine Learning Research},
  volume     = {205},
  abstract   = {Dealing with sparse rewards is a long-standing challenge in reinforcement learning (RL). Hindsight Experience Replay (HER) addresses this problem by reusing failed trajectories for one goal as successful trajectories for another. This allows for both a minimum density of reward and for generalization across multiple goals. However, this strategy is known to result in a biased value function, as the update rule underestimates the likelihood of bad outcomes in a stochastic environment. We propose an asymptotically unbiased importance-sampling-based algorithm to address this problem without sacrificing performance on deterministic environments. We show its effectiveness on a range of robotic systems, including challenging high dimensional stochastic environments.},
  file       = {:Schramm2023 - USHER_ Unbiased Sampling for Hindsight Experience Replay.pdf:PDF},
  groups     = {Herke's recommendations},
  pdf        = {https://proceedings.mlr.press/v205/schramm23a/schramm23a.pdf},
  readstatus = {read},
  url        = {https://proceedings.mlr.press/v205/schramm23a.html},
}

@InProceedings{Levy2019,
  author     = {Andrew Levy and George Dimitri Konidaris and Robert Platt Jr. and Kate Saenko},
  booktitle  = {7th International Conference on Learning Representations, {ICLR} 2019, New Orleans, LA, USA, May 6-9, 2019},
  title      = {Learning Multi-Level Hierarchies with Hindsight},
  year       = {2019},
  publisher  = {OpenReview.net},
  bibsource  = {dblp computer science bibliography, https://dblp.org},
  biburl     = {https://dblp.org/rec/conf/iclr/LevyKPS19.bib},
  file       = {:Levy2019 - Learning Multi Level Hierarchies with Hindsight.pdf:PDF},
  groups     = {Herke's recommendations},
  readstatus = {skimmed},
  timestamp  = {Tue, 19 Nov 2019 08:34:00 +0100},
  url        = {https://openreview.net/forum?id=ryzECoAcY7},
}

@InProceedings{Andrychowicz2017,
  author     = {Andrychowicz, Marcin and Wolski, Filip and Ray, Alex and Schneider, Jonas and Fong, Rachel and Welinder, Peter and McGrew, Bob and Tobin, Josh and Pieter Abbeel, OpenAI and Zaremba, Wojciech},
  booktitle  = {Advances in Neural Information Processing Systems},
  title      = {Hindsight Experience Replay},
  year       = {2017},
  editor     = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
  publisher  = {Curran Associates, Inc.},
  volume     = {30},
  file       = {:Andrychowicz2017 - Hindsight Experience Replay.pdf:PDF},
  groups     = {Herke's recommendations},
  readstatus = {read},
  url        = {https://proceedings.neurips.cc/paper/2017/file/453fadbd8a1a3af50a9df4df899537b5-Paper.pdf},
}

@Article{Yin2023,
  author        = {Dong Yin and Sridhar Thiagarajan and Nevena Lazic and Nived Rajaraman and Botao Hao and Csaba Szepesv{\'{a}}ri},
  journal       = {CoRR},
  title         = {Sample Efficient Deep Reinforcement Learning via Local Planning},
  year          = {2023},
  volume        = {abs/2301.12579},
  archiveprefix = {arXiv},
  bibsource     = {dblp computer science bibliography, https://dblp.org},
  biburl        = {https://dblp.org/rec/journals/corr/abs-2301-12579.bib},
  doi           = {10.48550/arXiv.2301.12579},
  eprint        = {2301.12579},
  file          = {:Yin2023 - Sample Efficient Deep Reinforcement Learning Via Local Planning.pdf:PDF},
}

'
@TechReport{Liu2022,
  author     = {Liu, Minghuan and Zhu, Menghui and Zhang, Weinan},
  title      = {Goal-{Conditioned} {Reinforcement} {Learning}: {Problems} and {Solutions}},
  year       = {2022},
  month      = sep,
  note       = {arXiv:2201.08299 [cs] type: article},
  abstract   = {Goal-conditioned reinforcement learning (GCRL), related to a set of complex RL problems, trains an agent to achieve different goals under particular scenarios. Compared to the standard RL solutions that learn a policy solely depending on the states or observations, GCRL additionally requires the agent to make decisions according to different goals. In this survey, we provide a comprehensive overview of the challenges and algorithms for GCRL. Firstly, we answer what the basic problems are studied in this field. Then, we explain how goals are represented and present how existing solutions are designed from different points of view. Finally, we make the conclusion and discuss potential future prospects that recent researches focus on.},
  annote     = {Comment: 10 pages, 3 figures, 2 tables. Published at IJCAI-ECAI 2022 (Survey Track). Add clarified words},
  doi        = {10.48550/arXiv.2201.08299},
  eprint     = {2201.08299},
  eprinttype = {arxiv},
  file       = {:Liu2022 - Goal Conditioned Reinforcement Learning_ Problems and Solutions.pdf:PDF},
  groups     = {Off-policy RL},
  keywords   = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
  school     = {arXiv},
  shorttitle = {Goal-{Conditioned} {Reinforcement} {Learning}},
}

'
@TechReport{Kong2023,
  author     = {Kong, Fang and Zhang, Xiangcheng and Wang, Baoxiang and Li, Shuai},
  title      = {Improved {Regret} {Bounds} for {Linear} {Adversarial} {MDPs} via {Linear} {Optimization}},
  year       = {2023},
  month      = feb,
  note       = {arXiv:2302.06834 [cs] type: article},
  abstract   = {Learning Markov decision processes (MDP) in an adversarial environment has been a challenging problem. The problem becomes even more challenging with function approximation, since the underlying structure of the loss function and transition kernel are especially hard to estimate in a varying environment. In fact, the state-of-the-art results for linear adversarial MDP achieve a regret of \${\textbackslash}tilde\{O\}(K{\textasciicircum}\{6/7\})\$ (\$K\$ denotes the number of episodes), which admits a large room for improvement. In this paper, we investigate the problem with a new view, which reduces linear MDP into linear optimization by subtly setting the feature maps of the bandit arms of linear optimization. This new technique, under an exploratory assumption, yields an improved bound of \${\textbackslash}tilde\{O\}(K{\textasciicircum}\{4/5\})\$ for linear adversarial MDP without access to a transition simulator. The new view could be of independent interest for solving other MDP problems that possess a linear structure.},
  doi        = {10.48550/arXiv.2302.06834},
  eprint     = {2302.06834},
  eprinttype = {arxiv},
  file       = {:Kong2023 - Improved Regret Bounds for Linear Adversarial MDPs Via Linear Optimization.pdf:PDF},
  groups     = {Reading Group},
  keywords   = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence},
  school     = {arXiv},
}

'
@InProceedings{Parr1997,
  author    = {Parr, Ronald and Russell, Stuart},
  booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
  title     = {Reinforcement {Learning} with {Hierarchies} of {Machines}},
  year      = {1997},
  publisher = {MIT Press},
  volume    = {10},
  abstract  = {We present a new approach to reinforcement learning in which the poli(cid:173) cies considered by the learning process are constrained by hierarchies of  partially specified machines.  This allows for the  use of prior knowledge  to reduce the search space and provides a framework in which knowledge  can  be  transferred  across  problems  and  in  which  component solutions  can be recombined to solve larger and more complicated problems.  Our  approach can be seen  as providing a link between reinforcement learn(cid:173) ing and "behavior-based" or "teleo-reactive" approaches to  control.  We  present provably  convergent algorithms  for  problem-solving and learn(cid:173) ing with hierarchical machines and demonstrate their effectiveness on a  problem with several thousand states.},
  file      = {:Parr1997 - Reinforcement Learning with Hierarchies of Machines.pdf:PDF},
  groups    = {Hierarchical RL},
  url       = {https://proceedings.neurips.cc/paper/1997/hash/5ca3e9b122f61f8f06494c97b1afccf3-Abstract.html},
  urldate   = {2023-03-27},
}

@InProceedings{Xie2023,
  author    = {Tengyang Xie and Dylan J Foster and Yu Bai and Nan Jiang and Sham M. Kakade},
  booktitle = {The Eleventh International Conference on Learning Representations},
  title     = {The Role of Coverage in Online Reinforcement Learning},
  year      = {2023},
  file      = {:Xie2023 - The Role of Coverage in Online Reinforcement Learning.pdf:PDF},
  groups    = {Reading Group},
  url       = {https://openreview.net/forum?id=LQIjzPdDt3q},
}

'
@Article{OrtizHaro2022,
  author   = {Ortiz-Haro, Joaquim and Karpas, Erez and Katz, Michael and Toussaint, Marc},
  journal  = {IEEE Robotics and Automation Letters},
  title    = {A {Conflict}-{Driven} {Interface} {Between} {Symbolic} {Planning} and {Nonlinear} {Constraint} {Solving}},
  year     = {2022},
  issn     = {2377-3766},
  month    = oct,
  number   = {4},
  pages    = {10518--10525},
  volume   = {7},
  abstract = {Robotic planning in real-world scenarios typically requires joint optimization of logic and continuous variables. A core challenge to combine the strengths of logic planners and continuous solvers is the design of an efficient interface that informs the logical search about continuous infeasibilities. In this paper we present a novel iterative algorithm that connects logic planning with nonlinear optimization through a bidirectional interface, achieved by the detection of minimal subsets of nonlinear constraints that are infeasible. The algorithm continuously builds a database of graphs that represent (in)feasible subsets of continuous variables and constraints, and encodes this knowledge in the logical description. As a foundation for this algorithm, we introduce Planning with Nonlinear Transition Constraints (PNTC), a novel planning formulation that clarifies the exact assumptions our algorithm requires and can be applied to model Task and Motion Planning (TAMP) efficiently. Our experimental results show that our framework significantly outperforms alternative optimization-based approaches for TAMP.},
  doi      = {10.1109/LRA.2022.3191948},
  file     = {:OrtizHaro2022 - A Conflict Driven Interface between Symbolic Planning and Nonlinear Constraint Solving.pdf:PDF},
  groups   = {Herke's recommendations},
  keywords = {Planning, Task analysis, Optimization, Trajectory, Manipulators, Collision avoidance, Search problems, Task and motion planning, task planning, manipulation planning},
}

'
@InProceedings{Nachum2018,
  author     = {Nachum, Ofir and Gu, Shixiang (Shane) and Lee, Honglak and Levine, Sergey},
  booktitle  = {Advances in {Neural} {Information} {Processing} {Systems}},
  title      = {Data-{Efficient} {Hierarchical} {Reinforcement} {Learning}},
  year       = {2018},
  publisher  = {Curran Associates, Inc.},
  volume     = {31},
  abstract   = {Hierarchical reinforcement learning (HRL) is a promising approach to extend traditional reinforcement learning (RL) methods to solve more complex tasks. Yet, the majority of current HRL methods require careful task-specific design and on-policy training, making them difficult to apply in real-world scenarios. In this paper, we study how we can develop HRL algorithms that are general, in that they do not make onerous additional assumptions beyond standard RL algorithms, and efficient, in the sense that they can be used with modest numbers of interaction samples, making them suitable for real-world problems such as robotic control. For generality, we develop a scheme where lower-level controllers are supervised with goals that are learned and proposed automatically by the higher-level controllers. To address efficiency, we propose to use off-policy experience for both higher- and lower-level training. This poses a considerable challenge, since changes to the lower-level behaviors change the action space for the higher-level policy, and we introduce an off-policy correction to remedy this challenge. This allows us to take advantage of recent advances in off-policy model-free RL to learn both higher and lower-level policies using substantially fewer environment interactions than on-policy algorithms. We find that our resulting HRL agent is generally applicable and highly sample-efficient. Our experiments show that our method can be used to learn highly complex behaviors for simulated robots, such as pushing objects and utilizing them to reach target locations, learning from only a few million samples, equivalent to a few days of real-time interaction. In comparisons with a  number of prior HRL methods, we find that our approach substantially outperforms previous state-of-the-art techniques.},
  file       = {:Nachum2018 - Data Efficient Hierarchical Reinforcement Learning.pdf:PDF},
  groups     = {Herke's recommendations, Hierarchical RL},
  readstatus = {skimmed},
  url        = {https://papers.nips.cc/paper_files/paper/2018/hash/e6384711491713d29bc63fc5eeb5ba4f-Abstract.html},
  urldate    = {2023-04-03},
}

@Article{FurelosBlanco2022,
  author        = {Daniel Furelos{-}Blanco and Mark Law and Anders Jonsson and Krysia Broda and Alessandra Russo},
  journal       = {CoRR},
  title         = {Hierarchies of Reward Machines},
  year          = {2022},
  volume        = {abs/2205.15752},
  archiveprefix = {arXiv},
  bibsource     = {dblp computer science bibliography, https://dblp.org},
  biburl        = {https://dblp.org/rec/journals/corr/abs-2205-15752.bib},
  doi           = {10.48550/arXiv.2205.15752},
  eprint        = {2205.15752},
  file          = {:FurelosBlanco2022 - Hierarchies of Reward Machines.pdf:PDF},
  groups        = {Reward machines, Hierarchical RL},
}

'
@Article{Icarte2022,
  author     = {Icarte, Rodrigo Toro and Klassen, Toryn Q. and Valenzano, Richard and McIlraith, Sheila A.},
  journal    = {Journal of Artificial Intelligence Research},
  title      = {Reward {Machines}: {Exploiting} {Reward} {Function} {Structure} in {Reinforcement} {Learning}},
  year       = {2022},
  issn       = {1076-9757},
  month      = jan,
  pages      = {173--208},
  volume     = {73},
  abstract   = {Reinforcement learning (RL) methods usually treat reward functions as black boxes. As such, these methods must extensively interact with the environment in order to discover rewards and optimal policies. In most RL applications, however, users have to program the reward function and, hence, there is the opportunity to make the reward function visible – to show the reward function’s code to the RL agent so it can exploit the function’s internal structure to learn optimal policies in a more sample efficient manner. In this paper, we show how to accomplish this idea in two steps. First, we propose reward machines, a type of finite state machine that supports the specification of reward functions while exposing reward function structure. We then describe different methodologies to exploit this structure to support learning, including automated reward shaping, task decomposition, and counterfactual reasoning with off-policy learning. Experiments on tabular and continuous domains, across different tasks and RL agents, show the benefits of exploiting reward structure with respect to sample efficiency and the quality of resultant policies. Finally, by virtue of being a form of finite state machine, reward machines have the expressive power of a regular language and as such support loops, sequences and conditionals, as well as the expression of temporally extended properties typical of linear temporal logic and non-Markovian reward specification.},
  copyright  = {Copyright (c)},
  doi        = {10.1613/jair.1.12440},
  file       = {:Icarte2022 - Reward Machines_ Exploiting Reward Function Structure in Reinforcement Learning.pdf:PDF},
  groups     = {Reward machines},
  keywords   = {reinforcement learning, automated reasoning, causality, knowledge representation},
  language   = {en},
  readstatus = {read},
  shorttitle = {Reward {Machines}},
  url        = {https://www.jair.org/index.php/jair/article/view/12440},
  urldate    = {2023-04-06},
}

'
@InProceedings{ToroIcarte2019,
  author    = {Toro Icarte, Rodrigo and Waldie, Ethan and Klassen, Toryn and Valenzano, Rick and Castro, Margarita and McIlraith, Sheila},
  booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
  title     = {Learning {Reward} {Machines} for {Partially} {Observable} {Reinforcement} {Learning}},
  year      = {2019},
  publisher = {Curran Associates, Inc.},
  volume    = {32},
  abstract  = {Reward Machines (RMs), originally proposed for specifying problems in Reinforcement Learning (RL), provide a structured, automata-based representation of a reward function that allows an agent to decompose problems into subproblems that can be efficiently learned using off-policy learning. Here we show that RMs can be learned from experience, instead of being specified by the user, and that the resulting problem decomposition can be used to effectively solve partially observable RL problems. We pose the task of learning RMs as a discrete optimization problem where the objective is to find an RM that decomposes the problem into a set of subproblems such that the combination of their optimal memoryless policies is an optimal policy for the original problem. We show the effectiveness of this approach on three partially observable domains, where it significantly outperforms A3C, PPO, and ACER, and discuss its advantages, limitations, and broader potential.},
  file      = {:ToroIcarte2019 - Learning Reward Machines for Partially Observable Reinforcement Learning.pdf:PDF},
  groups    = {Reward machines},
  url       = {https://papers.nips.cc/paper_files/paper/2019/hash/532435c44bec236b471a47a88d63513d-Abstract.html},
  urldate   = {2023-04-11},
}

'
@InProceedings{Vaezipoor2021,
  author     = {Vaezipoor, Pashootan and Li, Andrew C. and Icarte, Rodrigo A. Toro and Mcilraith, Sheila A.},
  title      = {{LTL2Action}: {Generalizing} {LTL} {Instructions} for {Multi}-{Task} {RL}},
  year       = {2021},
  month      = jul,
  pages      = {10497--10508},
  publisher  = {PMLR},
  abstract   = {We address the problem of teaching a deep reinforcement learning (RL) agent to follow instructions in multi-task environments. Instructions are expressed in a well-known formal language \{–\} linear temporal logic (LTL) \{–\} and can specify a diversity of complex, temporally extended behaviours, including conditionals and alternative realizations. Our proposed learning approach exploits the compositional syntax and the semantics of LTL, enabling our RL agent to learn task-conditioned policies that generalize to new instructions, not observed during training. To reduce the overhead of learning LTL semantics, we introduce an environment-agnostic LTL pretraining scheme which improves sample-efficiency in downstream environments. Experiments on discrete and continuous domains target combinatorial task sets of up to ∼1039∼1039{\textbackslash}sim10{\textasciicircum}\{39\} unique tasks and demonstrate the strength of our approach in learning to solve (unseen) tasks, given LTL instructions.},
  file       = {:Vaezipoor2021 - LTL2Action_ Generalizing LTL Instructions for Multi Task RL.pdf:PDF},
  issn       = {2640-3498},
  language   = {en},
  readstatus = {read},
  shorttitle = {{LTL2Action}},
  url        = {https://proceedings.mlr.press/v139/vaezipoor21a.html},
  urldate    = {2023-04-13},
}

'
@TechReport{Hessel2019,
  author   = {Hessel, Matteo and van Hasselt, Hado and Modayil, Joseph and Silver, David},
  title    = {On {Inductive} {Biases} in {Deep} {Reinforcement} {Learning}},
  year     = {2019},
  month    = jul,
  note     = {arXiv:1907.02908 [cs, stat] type: article},
  abstract = {Many deep reinforcement learning algorithms contain inductive biases that sculpt the agent's objective and its interface to the environment. These inductive biases can take many forms, including domain knowledge and pretuned hyper-parameters. In general, there is a trade-off between generality and performance when algorithms use such biases. Stronger biases can lead to faster learning, but weaker biases can potentially lead to more general algorithms. This trade-off is important because inductive biases are not free; substantial effort may be required to obtain relevant domain knowledge or to tune hyper-parameters effectively. In this paper, we re-examine several domain-specific components that bias the objective and the environmental interface of common deep reinforcement learning agents. We investigated whether the performance deteriorates when these components are replaced with adaptive solutions from the literature. In our experiments, performance sometimes decreased with the adaptive components, as one might expect when comparing to components crafted for the domain, but sometimes the adaptive components performed better. We investigated the main benefit of having fewer domain-specific components, by comparing the learning performance of the two systems on a different set of continuous control problems, without additional tuning of either system. As hypothesized, the system with adaptive components performed better on many of the new tasks.},
  doi      = {10.48550/arXiv.1907.02908},
  file     = {:Hessel2019 - On Inductive Biases in Deep Reinforcement Learning.pdf:PDF},
  keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Statistics - Machine Learning},
  school   = {arXiv},
  url      = {http://arxiv.org/abs/1907.02908},
  urldate  = {2023-04-13},
}

@Article{Alegre2023,
  author        = {Lucas Nunes Alegre and Ana L. C. Bazzan and Diederik M. Roijers and Ann Now{\'{e}} and Bruno C. da Silva},
  journal       = {CoRR},
  title         = {Sample-Efficient Multi-Objective Learning via Generalized Policy Improvement Prioritization},
  year          = {2023},
  volume        = {abs/2301.07784},
  archiveprefix = {arXiv},
  bibsource     = {dblp computer science bibliography, https://dblp.org},
  biburl        = {https://dblp.org/rec/journals/corr/abs-2301-07784.bib},
  doi           = {10.48550/arXiv.2301.07784},
  eprint        = {2301.07784},
  file          = {:Alegre2023 - Sample Efficient Multi Objective Learning Via Generalized Policy Improvement Prioritization.pdf:PDF},
  timestamp     = {Thu, 26 Jan 2023 15:26:31 +0100},
}

'
@InProceedings{Alegre2022,
  author     = {Alegre, Lucas Nunes and Bazzan, Ana and Silva, Bruno C. Da},
  title      = {Optimistic {Linear} {Support} and {Successor} {Features} as a {Basis} for {Optimal} {Policy} {Transfer}},
  year       = {2022},
  month      = jun,
  pages      = {394--413},
  publisher  = {PMLR},
  abstract   = {In many real-world applications, reinforcement learning (RL) agents might have to solve multiple tasks, each one typically modeled via a reward function. If reward functions are expressed linearly, and the agent has previously learned a set of policies for different tasks, successor features (SFs) can be exploited to combine such policies and identify reasonable solutions for new problems. However, the identified solutions are not guaranteed to be optimal. We introduce a novel algorithm that addresses this limitation. It allows RL agents to combine existing policies and directly identify optimal policies for arbitrary new problems, without requiring any further interactions with the environment. We first show (under mild assumptions) that the transfer learning problem tackled by SFs is equivalent to the problem of learning to optimize multiple objectives in RL. We then introduce an SF-based extension of the Optimistic Linear Support algorithm to learn a set of policies whose SFs form a convex coverage set. We prove that policies in this set can be combined via generalized policy improvement to construct optimal behaviors for any new linearly-expressible tasks, without requiring any additional training samples. We empirically show that our method outperforms state-of-the-art competing algorithms both in discrete and continuous domains under value function approximation.},
  file       = {:Alegre2022 - Optimistic Linear Support and Successor Features As a Basis for Optimal Policy Transfer.pdf:PDF},
  issn       = {2640-3498},
  language   = {en},
  readstatus = {read},
  url        = {https://proceedings.mlr.press/v162/alegre22a.html},
  urldate    = {2023-04-14},
}

'
@Article{Pateria2021,
  author     = {Pateria, Shubham and Subagdja, Budhitama and Tan, Ah-hwee and Quek, Chai},
  journal    = {ACM Computing Surveys},
  title      = {Hierarchical {Reinforcement} {Learning}: {A} {Comprehensive} {Survey}},
  year       = {2021},
  issn       = {0360-0300},
  month      = jun,
  number     = {5},
  pages      = {109:1--109:35},
  volume     = {54},
  abstract   = {Hierarchical Reinforcement Learning (HRL) enables autonomous decomposition of challenging long-horizon decision-making tasks into simpler subtasks. During the past years, the landscape of HRL research has grown profoundly, resulting in copious approaches. A comprehensive overview of this vast landscape is necessary to study HRL in an organized manner. We provide a survey of the diverse HRL approaches concerning the challenges of learning hierarchical policies, subtask discovery, transfer learning, and multi-agent learning using HRL. The survey is presented according to a novel taxonomy of the approaches. Based on the survey, a set of important open problems is proposed to motivate the future research in HRL. Furthermore, we outline a few suitable task domains for evaluating the HRL approaches and a few interesting examples of the practical applications of HRL in the Supplementary Material.},
  doi        = {10.1145/3453160},
  file       = {:Pateria2021 - Hierarchical Reinforcement Learning_ a Comprehensive Survey.pdf:PDF},
  keywords   = {hierarchical reinforcement learning taxonomy, hierarchical reinforcement learning survey, subtask discovery, skill discovery, Hierarchical reinforcement learning},
  readstatus = {read},
  shorttitle = {Hierarchical {Reinforcement} {Learning}},
  urldate    = {2023-04-18},
}

'
@InProceedings{Hunt2019,
  author    = {Hunt, Jonathan and Barreto, Andre and Lillicrap, Timothy and Heess, Nicolas},
  title     = {Composing {Entropic} {Policies} using {Divergence} {Correction}},
  year      = {2019},
  month     = may,
  pages     = {2911--2920},
  publisher = {PMLR},
  abstract  = {Composing skills mastered in one task to solve novel tasks promises dramatic improvements in the data efficiency of reinforcement learning. Here, we analyze two recent works composing behaviors represented in the form of action-value functions and show that they perform poorly in some situations. As part of this analysis, we extend an important generalization of policy improvement to the maximum entropy framework and introduce an algorithm for the practical implementation of successor features in continuous action spaces. Then we propose a novel approach which addresses the failure cases of prior work and, in principle, recovers the optimal policy during transfer. This method works by explicitly learning the (discounted, future) divergence between base policies. We study this approach in the tabular case and on non-trivial continuous control problems with compositional structure and show that it outperforms or matches existing methods across all tasks considered.},
  file      = {:Hunt2019 - Composing Entropic Policies Using Divergence Correction.pdf:PDF},
  issn      = {2640-3498},
  language  = {en},
  url       = {https://proceedings.mlr.press/v97/hunt19a.html},
  urldate   = {2023-04-19},
}

@InProceedings{Jiang2022,
  author    = {Jiang, Yiding and Liu, Evan and Eysenbach, Benjamin and Kolter, J. Zico and Finn, Chelsea},
  booktitle = {Advances in Neural Information Processing Systems},
  title     = {Learning Options via Compression},
  year      = {2022},
  editor    = {S. Koyejo and S. Mohamed and A. Agarwal and D. Belgrave and K. Cho and A. Oh},
  pages     = {21184--21199},
  publisher = {Curran Associates, Inc.},
  volume    = {35},
  file      = {:Jiang2022 - Learning Options Via Compression.pdf:PDF},
  url       = {https://proceedings.neurips.cc/paper_files/paper/2022/file/8567a53e58a9fa4823af356c76ed943c-Paper-Conference.pdf},
}

'
@InProceedings{Wen2020,
  author     = {Wen, Zheng and Precup, Doina and Ibrahimi, Morteza and Barreto, Andre and Van Roy, Benjamin and Singh, Satinder},
  booktitle  = {Advances in {Neural} {Information} {Processing} {Systems}},
  title      = {On {Efficiency} in {Hierarchical} {Reinforcement} {Learning}},
  year       = {2020},
  pages      = {6708--6718},
  publisher  = {Curran Associates, Inc.},
  volume     = {33},
  abstract   = {Hierarchical Reinforcement Learning (HRL) approaches promise to provide more efficient solutions to sequential decision making problems, both in terms of statistical as well as computational efficiency. While this has been demonstrated empirically over time in a variety of tasks, theoretical results quantifying the benefits of such methods are still few and far between. In this paper, we discuss the kind of structure in a Markov decision process which gives rise to efficient HRL methods. Specifically, we formalize the intuition that HRL can exploit well repeating "subMDPs", with similar reward and transition structure. We show that, under reasonable assumptions, a model-based Thompson sampling-style HRL algorithm that exploits this structure is statistically efficient, as established through a finite-time regret bound. We also establish conditions under which planning with structure-induced options is near-optimal and computationally efficient.},
  file       = {:Wen2020 - On Efficiency in Hierarchical Reinforcement Learning.pdf:PDF},
  readstatus = {skimmed},
  url        = {https://proceedings.neurips.cc/paper/2020/hash/4a5cfa9281924139db466a8a19291aff-Abstract.html},
  urldate    = {2023-04-20},
}

'
@InProceedings{Haarnoja2017,
  author    = {Haarnoja, Tuomas and Tang, Haoran and Abbeel, Pieter and Levine, Sergey},
  title     = {Reinforcement {Learning} with {Deep} {Energy}-{Based} {Policies}},
  year      = {2017},
  month     = jul,
  pages     = {1352--1361},
  publisher = {PMLR},
  abstract  = {We propose a method for learning expressive energy-based policies for continuous states and actions, which has been feasible only in tabular domains before. We apply our method to learning maximum entropy policies, resulting into a new algorithm, called soft Q-learning, that expresses the optimal policy via a Boltzmann distribution. We use the recently proposed amortized Stein variational gradient descent to learn a stochastic sampling network that approximates samples from this distribution. The benefits of the proposed algorithm include improved exploration and compositionality that allows transferring skills between tasks, which we confirm in simulated experiments with swimming and walking robots. We also draw a connection to actor-critic methods, which can be viewed performing approximate inference on the corresponding energy-based model.},
  comment   = {Section 3.3 is tought to understand. Should ask to Vicenç and Anders to know more about it.},
  file      = {:Haarnoja2017 - Reinforcement Learning with Deep Energy Based Policies.pdf:PDF},
  issn      = {2640-3498},
  language  = {en},
  url       = {https://proceedings.mlr.press/v70/haarnoja17a.html},
  urldate   = {2023-04-21},
}

'
@InProceedings{NangueTasse2020,
  author    = {Nangue Tasse, Geraud and James, Steven and Rosman, Benjamin},
  booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
  title     = {A {Boolean} {Task} {Algebra} for {Reinforcement} {Learning}},
  year      = {2020},
  pages     = {9497--9507},
  publisher = {Curran Associates, Inc.},
  volume    = {33},
  abstract  = {The ability to compose learned skills to solve new tasks is an important property for lifelong-learning agents. In this work we formalise the logical composition of tasks as a Boolean algebra. This allows us to formulate new tasks in terms of the negation, disjunction and conjunction of a set of base tasks. We then show that by learning goal-oriented value functions and restricting the transition dynamics of the tasks, an agent can solve these new tasks with no further learning. We prove that by composing these value functions in specific ways, we immediately recover the optimal policies for all tasks expressible under the Boolean algebra. We verify our approach in two domains---including a high-dimensional video game environment requiring function approximation---where an agent first learns a set of base skills, and then composes them to solve a super-exponential number of new tasks.},
  file      = {:NangueTasse2020 - A Boolean Task Algebra for Reinforcement Learning.pdf:PDF},
  url       = {https://proceedings.neurips.cc/paper/2020/hash/6ba3af5d7b2790e73f0de32e5c8c1798-Abstract.html},
  urldate   = {2023-04-21},
}

'
@InProceedings{Gimelfarb2021,
  author    = {Gimelfarb, Michael and Barreto, Andre and Sanner, Scott and Lee, Chi-Guhn},
  booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
  title     = {Risk-{Aware} {Transfer} in {Reinforcement} {Learning} using {Successor} {Features}},
  year      = {2021},
  pages     = {17298--17310},
  publisher = {Curran Associates, Inc.},
  volume    = {34},
  abstract  = {Sample efficiency and risk-awareness are central to the development of practical reinforcement learning (RL) for complex decision-making. The former can be addressed by transfer learning, while the latter by optimizing some utility function of the return. However, the problem of transferring skills in a risk-aware manner is not well-understood. In this paper, we address the problem of transferring policies between tasks in a common domain that differ only in their reward functions, in which risk is measured by the variance of reward streams. Our approach begins by extending the idea of generalized policy improvement to maximize entropic utilities, thus extending the dynamic programming's policy improvement operation to sets of policies {\textbackslash}emph\{and\} levels of risk-aversion. Next, we extend the idea of successor features (SF), a value function representation that decouples the environment dynamics from the rewards, to capture the variance of returns. Our resulting risk-aware successor features (RaSF) integrate seamlessly within the RL framework, inherit the superior task generalization ability of SFs, while incorporating risk into the decision-making. Experiments on a discrete navigation domain and control of a simulated robotic arm demonstrate the ability of RaSFs to outperform alternative methods including SFs, when taking the risk of the learned policies into account.},
  file      = {:Gimelfarb2021 - Risk Aware Transfer in Reinforcement Learning Using Successor Features.pdf:PDF},
  url       = {https://proceedings.neurips.cc/paper/2021/hash/90610aa0e24f63ec6d2637e06f9b9af2-Abstract.html},
  urldate   = {2023-04-24},
}

'
@TechReport{Adamczyk2023,
  author   = {Adamczyk, Jacob and Tiomkin, Stas and Kulkarni, Rahul},
  title    = {Compositionality and {Bounds} for {Optimal} {Value} {Functions} in {Reinforcement} {Learning}},
  year     = {2023},
  month    = feb,
  note     = {arXiv:2302.09676 [cs] type: article},
  abstract = {An agent's ability to reuse solutions to previously solved problems is critical for learning new tasks efficiently. Recent research using composition of value functions in reinforcement learning has shown that agents can utilize solutions of primitive tasks to obtain solutions for exponentially many new tasks. However, previous work has relied on restrictive assumptions on the dynamics, the method of composition, and the structure of reward functions. Here we consider the case of general composition functions without any restrictions on the structure of reward functions, applicable to both deterministic and stochastic dynamics. For this general setup, we provide bounds on the corresponding optimal value functions and characterize the value of corresponding policies. The theoretical results derived lead to improvements in training for both entropy-regularized and standard reinforcement learning, which we validate with numerical simulations.},
  doi      = {10.48550/arXiv.2302.09676},
  file     = {:Adamczyk2023 - Compositionality and Bounds for Optimal Value Functions in Reinforcement Learning.pdf:PDF},
  keywords = {Computer Science - Machine Learning},
  school   = {arXiv},
  url      = {http://arxiv.org/abs/2302.09676},
  urldate  = {2023-04-24},
}

'
@InProceedings{Jothimurugan2021,
  author    = {Jothimurugan, Kishor and Bansal, Suguman and Bastani, Osbert and Alur, Rajeev},
  booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
  title     = {Compositional {Reinforcement} {Learning} from {Logical} {Specifications}},
  year      = {2021},
  pages     = {10026--10039},
  publisher = {Curran Associates, Inc.},
  volume    = {34},
  abstract  = {We study the problem of learning control policies for complex tasks given by logical specifications. Recent approaches automatically generate a reward function from a given specification and use a suitable reinforcement learning algorithm to learn a policy that maximizes the expected reward. These approaches, however, scale poorly to complex tasks that require high-level planning. In this work, we develop a compositional learning approach, called DIRL, that interleaves high-level planning and reinforcement learning. First, DIRL encodes the specification as an abstract graph; intuitively, vertices and edges of the graph correspond to regions of the state space and simpler sub-tasks, respectively. Our approach then incorporates reinforcement learning to learn neural network policies for each edge (sub-task) within a Dijkstra-style planning algorithm to compute a high-level plan in the graph. An evaluation of the proposed approach on a set of challenging control benchmarks with continuous state and action spaces demonstrates that it outperforms state-of-the-art baselines.},
  file      = {:Jothimurugan2021 - Compositional Reinforcement Learning from Logical Specifications.pdf:PDF},
  url       = {https://papers.nips.cc/paper_files/paper/2021/hash/531db99cb00833bcd414459069dc7387-Abstract.html},
  urldate   = {2023-04-24},
}

'
@InProceedings{Jothimurugan2021a,
  author    = {Jothimurugan, Kishor and Bastani, Osbert and Alur, Rajeev},
  title     = {Abstract {Value} {Iteration} for {Hierarchical} {Reinforcement} {Learning}},
  year      = {2021},
  month     = mar,
  pages     = {1162--1170},
  publisher = {PMLR},
  abstract  = {We propose a novel hierarchical reinforcement learning framework for control with continuous state and action spaces. In our framework, the user specifies subgoal regions which are subsets of states; then, we (i) learn options that serve as transitions between these subgoal regions, and (ii) construct a high-level plan in the resulting abstract decision process (ADP). A key challenge is that the ADP may not be Markov; we propose two algorithms for planning in the ADP that address this issue. Our first algorithm is conservative, allowing us to prove theoretical guarantees on its performance, which help inform the design of subgoal regions. Our second algorithm is a practical one that interweaves planning at the abstract level and learning at the concrete level. In our experiments, we demonstrate that our approach outperforms state-of-the-art hierarchical reinforcement learning algorithms on several challenging benchmarks.},
  file      = {:Jothimurugan2021a - Abstract Value Iteration for Hierarchical Reinforcement Learning.pdf:PDF},
  issn      = {2640-3498},
  language  = {en},
  url       = {https://proceedings.mlr.press/v130/jothimurugan21a.html},
  urldate   = {2023-04-24},
}

'
@InProceedings{Fruit2017a,
  author    = {Fruit, Ronan and Lazaric, Alessandro},
  title     = {Exploration-{Exploitation} in {MDPs} with {Options}},
  year      = {2017},
  month     = apr,
  pages     = {576--584},
  publisher = {PMLR},
  abstract  = {While a large body of empirical results show that temporally-extended actions and options may significantly affect the learning performance of an agent, the theoretical understanding of how and when options can be beneficial in online reinforcement learning is relatively limited. In this paper, we derive an upper and lower bound on the regret of a variant of UCRL using options. While we first analyze the algorithm in the general case of semi-Markov decision processes (SMDPs), we show how these results can be translated to the specific case of MDPs with options and we illustrate simple scenarios in which the regret of learning with options can be provably much smaller than the regret suffered when learning with primitive actions.},
  file      = {:fruit_exploration-exploitation_2017 - Exploration Exploitation in MDPs with Options.pdf:PDF;:fruit_exploration-exploitation_2017 - Exploration Exploitation in MDPs with Options (1).pdf:PDF},
  issn      = {2640-3498},
  language  = {en},
  url       = {https://proceedings.mlr.press/v54/fruit17a.html},
  urldate   = {2023-04-25},
}

'
@Article{Kappen2012,
  author   = {Kappen, Hilbert J. and Gómez, Vicenç and Opper, Manfred},
  journal  = {Machine Learning},
  title    = {Optimal control as a graphical model inference problem},
  year     = {2012},
  issn     = {1573-0565},
  month    = may,
  number   = {2},
  pages    = {159--182},
  volume   = {87},
  abstract = {We reformulate a class of non-linear stochastic optimal control problems introduced by Todorov (in Advances in Neural Information Processing Systems, vol. 19, pp. 1369–1376, 2007) as a Kullback-Leibler (KL) minimization problem. As a result, the optimal control computation reduces to an inference computation and approximate inference methods can be applied to efficiently compute approximate optimal controls. We show how this KL control theory contains the path integral control method as a special case. We provide an example of a block stacking task and a multi-agent cooperative game where we demonstrate how approximate inference can be successfully applied to instances that are too complex for exact computation. We discuss the relation of the KL control approach to other inference approaches to control.},
  doi      = {10.1007/s10994-012-5278-7},
  file     = {:Kappen2012 - Optimal Control As a Graphical Model Inference Problem.pdf:PDF},
  keywords = {Optimal control, Uncontrolled dynamics, Kullback-Leibler divergence, Graphical model, Approximate inference, Cluster variation method, Belief propagation},
  language = {en},
  url      = {https://doi.org/10.1007/s10994-012-5278-7},
  urldate  = {2023-04-29},
}

@Comment{jabref-meta: databaseType:bibtex;}

@Comment{jabref-meta: grouping:
0 AllEntriesGroup:;
1 StaticGroup:Average reward\;0\;0\;0x990000ff\;\;\;;
1 StaticGroup:Entropy-regularized MDPs\;0\;0\;0x008080ff\;\;\;;
1 StaticGroup:Herke's recommendations\;0\;1\;0x00ff00ff\;\;\;;
1 StaticGroup:Hierarchical RL\;0\;0\;0x1a3399ff\;\;\;;
1 StaticGroup:Off-policy RL\;0\;0\;0x996699ff\;\;\;;
1 StaticGroup:Policy gradients\;0\;0\;0xe6e64dff\;\;\;;
1 StaticGroup:Reading Group\;0\;1\;0xff00ffff\;ACCOUNT_SUPERVISOR_CIRCLE\;\;;
1 StaticGroup:Reward machines\;0\;0\;0xff0000ff\;ACCOUNT_ALERT_OUTLINE\;\;;
}

@Comment{jabref-meta: saveActions:disabled;
all-text-fields[identity]
date[normalize_date]
month[normalize_month]
pages[normalize_page_numbers]
;}
